# Worth-Reading-ML-Papers

Topic | Tags | Link
--|--|--
Xception: Depth Wise Convolution | faster-Inference, operation-reduction, Inception | [arxiv](https://arxiv.org/pdf/1610.02357.pdf)
Quantisation | model-compression, AlexNet, ImageNet | 
Model Pruning | model-compression, AlexNet, GoogleNet |
Hashing Trick Compressing | model-compression, weight-sharing, HashedNets | [github](https://github.com/pra-dan/Worth-Reading-ML-Papers/blob/master/some-of-the-papers/8compressing-neural-networks-with-the-hashing-trick.pdf)
K-Means, Huffman Compressing | model-compression, weight-sharing, k-means, deep-compression | [github](https://github.com/pra-dan/Worth-Reading-ML-Papers/blob/master/some-of-the-papers/9deep-compression-compressing-deep-neural-networks-with-pruning-trained-quantization-and-huffman-coding.pdf)
Distilling the Knowledge in a Neural Network | knowledge-distillation | [arxiv](https://arxiv.org/pdf/1503.02531.pdf) 
Improving ImageNet accuracy using knowledge distillation |knowledge-distillation, ImageNet | [github](https://github.com/pra-dan/Worth-Reading-ML-Papers/blob/master/some-of-the-papers/11self-training-with-noisy-student-improves-imagenet-classification.pdf)
Distillation + Quantization | knowledge-distillation, model-compression | [arxiv](https://arxiv.org/pdf/1802.05668.pdf)
Distillation + Pruning | nowledge-distillation, model-compression | [arxiv](https://arxiv.org/pdf/1801.05787.pdf)
